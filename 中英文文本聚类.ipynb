{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc32e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "# tokenizer = XLMRobertaTokenizer.from_pretrained('D:\\\\code\\\\bert_model\\\\pre_trained\\\\distiluse-base-multilingual-cased-v1')\n",
    "# model = XLMRobertaModel.from_pretrained('D:\\\\code\\\\bert_model\\\\pre_trained\\\\distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "#model2\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#pretrained_model\n",
    "#distiluse-base-multilingual-cased-v1\n",
    "#distiluse-base-multilingual-cased-v2\n",
    "#paraphrase-multilingual-MiniLM-L12-v\n",
    "#paraphrase-multilingual-mpnet-base-v2\n",
    "# model = SentenceTransformer(\"D:\\\\code\\\\bert_model\\\\pre_trained\\\\distiluse-base-multilingual-cased-v2\")\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbd721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path2=\"D:\\\\code\\\\git-repository\\\\bert\\\\学者收集合并\"\n",
    "# files = os.listdir(path2)\n",
    "#mac\n",
    "path = \"./学者收集合并\"\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ed3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_abstracts_to_dict(path, file):\n",
    "    abstracts_dict = {}\n",
    "    article_dict = {}\n",
    "    for file_name in file:\n",
    "        try:\n",
    "            file_path = f\"{path}//{file_name}\"  \n",
    "            df = pd.read_excel(file_path)  \n",
    "            abstracts_list = []\n",
    "            article_list = []\n",
    "            for index,row in df.iterrows():\n",
    "                try:\n",
    "                    if np.isnan(row['Abstract']):\n",
    "                        continue\n",
    "                except:\n",
    "                    abstracts_list.append(row['Abstract'])\n",
    "                    article_list.append(row['Article Title'])\n",
    "                abstracts_dict[file_name.rsplit('.', 1)[0]] = abstracts_list \n",
    "                article_dict[file_name.rsplit('.', 1)[0]] = article_list\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return article_dict,abstracts_dict\n",
    "\n",
    "\n",
    "article_dict,abstracts_dict = extract_abstracts_to_dict(path, files)\n",
    "article_list = []\n",
    "abstracts_list = []\n",
    "for item in article_dict.values():\n",
    "    article_list.extend(item)\n",
    "\n",
    "for item in abstracts_dict.values():\n",
    "    abstracts_list.extend(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012eef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_zhouxiao = abstracts_dict.get('周潇', [])\n",
    "article_zhouxiao = article_dict.get('周潇', [])\n",
    "print(f\"'周潇'的abstract列表: {abstracts_zhouxiao[:3]}\") \n",
    "print(f\"'周潇'的article列表: {article_zhouxiao}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676c098",
   "metadata": {},
   "source": [
    "数据清洗，去掉【结果】等没用处的字段，并去掉空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import re\n",
    "# pattern = r\"\\[.*?\\]|【.*?】\"\n",
    "# cleaned_abstracts_dict = {}\n",
    "# for key, abstracts in abstracts_dict.items():\n",
    "#     cleaned_abstracts = [re.sub(pattern, \"\", abstract) for abstract in abstracts if isinstance(abstract, str) and abstract]\n",
    "#     cleaned_abstracts_dict[key] = cleaned_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aae9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def text_to_embedding(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, add_special_tokens=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()} \n",
    "        with torch.no_grad():  \n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 1:-1, :].cpu() \n",
    "        return embeddings\n",
    "    except:\n",
    "        print(text)\n",
    "\n",
    "embedded_abstracts_dict = {}\n",
    "\n",
    "# for key, texts in cleaned_abstracts_dict.items():\n",
    "#     embedded_texts = [text_to_embedding(text) for text in texts]\n",
    "#     embedded_abstracts_dict[key] = embedded_texts\n",
    "\n",
    "for key, texts in abstracts_dict.items():\n",
    "    embedded_texts = [text_to_embedding(text) for text in texts]\n",
    "    embedded_abstracts_dict[key] = embedded_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2\n",
    "embedded_abstracts_dict = {}\n",
    "for key, texts in abstracts_dict.items():\n",
    "    embedded_texts = []\n",
    "    for text in texts:\n",
    "        embedding = model.encode(text)\n",
    "        embedded_texts.append(embedding)\n",
    "    embedded_abstracts_dict[key] = embedded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1984b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_abstracts_dict['周潇'][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "all_embeddings = []\n",
    "labels = []\n",
    "for label, tensors in embedded_abstracts_dict.items():\n",
    "    for tensor in tensors:\n",
    "        mean_embedding = tensor.mean(dim=1).squeeze(0).numpy()\n",
    "        all_embeddings.append(mean_embedding)\n",
    "        labels.append(label)\n",
    "embeddings_array = np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "all_embeddings = []\n",
    "labels = []\n",
    "for label, tensors in embedded_abstracts_dict.items():\n",
    "    for tensor in tensors:\n",
    "        all_embeddings.append(tensor)\n",
    "        labels.append(label)\n",
    "embeddings_array = np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53801c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2,init='pca')\n",
    "reduced_embeddings = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "label_to_color = {label: color for label, color in zip(unique_labels, colors)}\n",
    "\n",
    "for label in unique_labels:\n",
    "    indices = [i for i, l in enumerate(labels) if l == label]\n",
    "    plt.scatter(reduced_embeddings[indices, 0], reduced_embeddings[indices, 1], label=label, color=label_to_color[label])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6314d",
   "metadata": {},
   "source": [
    "## 可以看到，效果很差，几乎等于没做，大家的研究方向虽然相似，但是整篇段落摘要进行嵌入，粒度太粗，所以还是打算基于关键主题和词topic来做"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc291a5",
   "metadata": {},
   "source": [
    "# Kmeans聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b423b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "n = 50\n",
    "SSE = []  # 存放每次结果的误差平方和\n",
    "for k in range(1,n):\n",
    "    estimator = KMeans(n_clusters=k)  # 构造聚类器\n",
    "    estimator.fit(embeddings_array)\n",
    "    SSE.append(estimator.inertia_)\n",
    "X = range(1,n)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "plt.plot(X,SSE,'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "Scores = []  # 存放轮廓系数\n",
    "for k in range(2,50):\n",
    "    estimator = KMeans(n_clusters=k)  # 构造聚类器\n",
    "    estimator.fit(embeddings_array)\n",
    "    Scores.append(silhouette_score(embeddings_array,estimator.labels_,metric='euclidean'))\n",
    "X = range(2,50)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('轮廓系数')\n",
    "plt.plot(X,Scores,'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e084912",
   "metadata": {},
   "source": [
    "### 由图可见最佳聚类就是2类，至少在该模型的结果下\n",
    "### 接下来就需要研究该模型的分类效果是否正常，即查看在该分类下，两类论文是否呈现明显不同的主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c562f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = 2\n",
    "\n",
    "estimator = KMeans(n_clusters=cluster_num,random_state=0)\n",
    "estimator.fit(embeddings_array)\n",
    "clusers = []\n",
    "for i in range(cluster_num):\n",
    "    clusers.append([])\n",
    "\n",
    "for index in range(len(estimator.labels_)):\n",
    "    clusers[estimator.labels_[index]].append(article_list[index])\n",
    "\n",
    "n = 1\n",
    "for articles in clusers:\n",
    "    df = pd.DataFrame(articles, columns=['articles'])\n",
    "    df.to_excel(f\"article_class_{n}.xlsx\", index=False)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21987ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ffa66a8",
   "metadata": {},
   "source": [
    "## 使用sci-bert和Key-bart方法 (不同于bert 使用了编码器-解码器结构，bert只使用编码器，更适合于txt2txt的文本生成，抽取论文主题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2625f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r\"A:\\新下载\\1712840133188.png\",width=1000,height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('C:\\\\Users\\\\54758\\\\bert\\\\sic-bert')\n",
    "model = AutoModel.from_pretrained('C:\\\\Users\\\\54758\\\\bert\\\\sic-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9515287",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"C:\\\\Users\\\\54758\\\\bert\\\\keybart\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"C:\\\\Users\\\\54758\\\\bert\\\\keybart\")\n",
    "# 数据\n",
    "data=cleaned_abstracts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0536d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a94fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, tokenizer, model, device):\n",
    "    inputs = tokenizer.encode(\"extract keywords: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model.generate(inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "    outputs = outputs.to('cpu')\n",
    "    keywords = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    keywords_list = keywords.split(',')\n",
    "    return keywords_list\n",
    "for scholar, papers in data.items():\n",
    "    print(f\"学者：{scholar}\")\n",
    "    for paper in papers:\n",
    "        keywords = extract_keywords(paper, tokenizer, model,device)\n",
    "        print(f\"论文关键词：{keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822eb3ec",
   "metadata": {},
   "source": [
    "## 这个模型似乎并不能很好处理中文，中文得想办法使用其他方法，先搞英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771406a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_chinese(text):\n",
    "    return any('\\u4e00' <= char <= '\\u9fff' for char in text)# 检查文本是否包含中文字符\n",
    "def extract_keywords2(text, tokenizer, model, device):\n",
    "    if contains_chinese(text):\n",
    "        return []#去除中文部分\n",
    "    inputs = tokenizer.encode(\"extract keywords: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    outputs = model.generate(inputs, max_length=50, num_beams=5, early_stopping=True).to('cpu')\n",
    "    keywords = tokenizer.decode(outputs[0], skip_special_tokens=True).split(',')\n",
    "    return keywords\n",
    "\n",
    "scholars_keywords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for scholar, papers in data.items():\n",
    "    all_keywords = []\n",
    "    for paper in papers:\n",
    "        keywords = extract_keywords2(paper, tokenizer, model, device)\n",
    "        all_keywords.extend(keywords) \n",
    "    scholars_keywords[scholar] = all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scholar, keywords in scholars_keywords.items():\n",
    "#     keyword_counts = Counter(keywords)\n",
    "#     sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "#     print(f\"学者：{scholar}\")\n",
    "#     for keyword, freq in sorted_keywords:\n",
    "#         print(f\"{keyword}: {freq}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10869f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scholar, keywords in scholars_keywords.items():\n",
    "    all_keywords = [keyword for i in keywords for keyword in i.split(';') if keyword]  # 分割每个字符串，忽略空字符串\n",
    "    keyword_counts = Counter(all_keywords)\n",
    "    sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"学者：{scholar}\")\n",
    "    for keyword, freq in sorted_keywords:\n",
    "        print(f\"{keyword}: {freq}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ddc19",
   "metadata": {},
   "source": [
    "### 虽然都提取出来了，但每个的词频似乎都不太高，拿来聚类效果一般不明显"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae173a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
