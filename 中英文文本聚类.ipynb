{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc32e44",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XLMRobertaModel, XLMRobertaTokenizer\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m XLMRobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbert_model\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpre_trained\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdistiluse-base-multilingual-cased-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m XLMRobertaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbert_model\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpre_trained\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdistiluse-base-multilingual-cased-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\software\\miniconda\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32md:\\software\\miniconda\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32md:\\software\\miniconda\\Lib\\site-packages\\transformers\\utils\\__init__.py:33\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     27\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     32\u001b[0m )\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     ContextManagers,\n\u001b[0;32m     35\u001b[0m     ExplicitEnum,\n\u001b[0;32m     36\u001b[0m     ModelOutput,\n\u001b[0;32m     37\u001b[0m     PaddingStrategy,\n\u001b[0;32m     38\u001b[0m     TensorType,\n\u001b[0;32m     39\u001b[0m     add_model_info_to_auto_map,\n\u001b[0;32m     40\u001b[0m     cached_property,\n\u001b[0;32m     41\u001b[0m     can_return_loss,\n\u001b[0;32m     42\u001b[0m     expand_dims,\n\u001b[0;32m     43\u001b[0m     find_labels,\n\u001b[0;32m     44\u001b[0m     flatten_dict,\n\u001b[0;32m     45\u001b[0m     infer_framework,\n\u001b[0;32m     46\u001b[0m     is_jax_tensor,\n\u001b[0;32m     47\u001b[0m     is_numpy_array,\n\u001b[0;32m     48\u001b[0m     is_tensor,\n\u001b[0;32m     49\u001b[0m     is_tf_symbolic_tensor,\n\u001b[0;32m     50\u001b[0m     is_tf_tensor,\n\u001b[0;32m     51\u001b[0m     is_torch_device,\n\u001b[0;32m     52\u001b[0m     is_torch_dtype,\n\u001b[0;32m     53\u001b[0m     is_torch_tensor,\n\u001b[0;32m     54\u001b[0m     reshape,\n\u001b[0;32m     55\u001b[0m     squeeze,\n\u001b[0;32m     56\u001b[0m     strtobool,\n\u001b[0;32m     57\u001b[0m     tensor_size,\n\u001b[0;32m     58\u001b[0m     to_numpy,\n\u001b[0;32m     59\u001b[0m     to_py_obj,\n\u001b[0;32m     60\u001b[0m     transpose,\n\u001b[0;32m     61\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     64\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     65\u001b[0m     HF_MODULES_CACHE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m     try_to_load_from_cache,\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[0;32m     95\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m     torch_only_method,\n\u001b[0;32m    205\u001b[0m )\n",
      "File \u001b[1;32md:\\software\\miniconda\\Lib\\site-packages\\transformers\\utils\\generic.py:465\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32md:\\software\\miniconda\\Lib\\site-packages\\torch\\__init__.py:125\u001b[0m\n\u001b[0;32m    123\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 125\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('D:\\\\code\\\\bert_model\\\\pre_trained\\\\xlm-roberta')\n",
    "model = XLMRobertaModel.from_pretrained('D:\\\\code\\\\bert_model\\\\pre_trained\\\\xlm-roberta')\n",
    "\n",
    "#model2\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#pretrained_model\n",
    "#distiluse-base-multilingual-cased-v1\n",
    "#distiluse-base-multilingual-cased-v2\n",
    "#paraphrase-multilingual-MiniLM-L12-v\n",
    "#paraphrase-multilingual-mpnet-base-v2\n",
    "# model = SentenceTransformer(\"D:\\\\code\\\\bert_model\\\\pre_trained\\\\distiluse-base-multilingual-cased-v2\")\n",
    "# model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbd721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path2=\"D:\\\\code\\\\git-repository\\\\bert\\\\学者收集合并\"\n",
    "# files = os.listdir(path2)\n",
    "#mac\n",
    "path = \"./学者收集合并\"\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ed3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_abstracts_to_dict(path, file):\n",
    "    abstracts_dict = {}\n",
    "    article_dict = {}\n",
    "    for file_name in file:\n",
    "        try:\n",
    "            file_path = f\"{path}//{file_name}\"  \n",
    "            df = pd.read_excel(file_path)  \n",
    "            abstracts_list = []\n",
    "            article_list = []\n",
    "            for index,row in df.iterrows():\n",
    "                try:\n",
    "                    if np.isnan(row['Abstract']):\n",
    "                        continue\n",
    "                except:\n",
    "                    abstracts_list.append(row['Abstract'])\n",
    "                    article_list.append(row['Article Title'])\n",
    "                abstracts_dict[file_name.rsplit('.', 1)[0]] = abstracts_list \n",
    "                article_dict[file_name.rsplit('.', 1)[0]] = article_list\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return article_dict,abstracts_dict\n",
    "\n",
    "\n",
    "article_dict,abstracts_dict = extract_abstracts_to_dict(path, files)\n",
    "article_list = []\n",
    "abstracts_list = []\n",
    "for item in article_dict.values():\n",
    "    article_list.extend(item)\n",
    "\n",
    "for item in abstracts_dict.values():\n",
    "    abstracts_list.extend(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012eef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_zhouxiao = abstracts_dict.get('周潇', [])\n",
    "article_zhouxiao = article_dict.get('周潇', [])\n",
    "print(f\"'周潇'的abstract列表: {abstracts_zhouxiao[:3]}\") \n",
    "print(f\"'周潇'的article列表: {article_zhouxiao}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676c098",
   "metadata": {},
   "source": [
    "数据清洗，去掉【结果】等没用处的字段，并去掉空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import re\n",
    "# pattern = r\"\\[.*?\\]|【.*?】\"\n",
    "# cleaned_abstracts_dict = {}\n",
    "# for key, abstracts in abstracts_dict.items():\n",
    "#     cleaned_abstracts = [re.sub(pattern, \"\", abstract) for abstract in abstracts if isinstance(abstract, str) and abstract]\n",
    "#     cleaned_abstracts_dict[key] = cleaned_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aae9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def text_to_embedding(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, add_special_tokens=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()} \n",
    "        with torch.no_grad():  \n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 1:-1, :].cpu() \n",
    "        return embeddings\n",
    "    except:\n",
    "        print(text)\n",
    "\n",
    "embedded_abstracts_dict = {}\n",
    "\n",
    "# for key, texts in cleaned_abstracts_dict.items():\n",
    "#     embedded_texts = [text_to_embedding(text) for text in texts]\n",
    "#     embedded_abstracts_dict[key] = embedded_texts\n",
    "\n",
    "for key, texts in abstracts_dict.items():\n",
    "    embedded_texts = [text_to_embedding(text) for text in texts]\n",
    "    embedded_abstracts_dict[key] = embedded_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2\n",
    "embedded_abstracts_dict = {}\n",
    "for key, texts in abstracts_dict.items():\n",
    "    embedded_texts = []\n",
    "    for text in texts:\n",
    "        embedding = model.encode(text)\n",
    "        embedded_texts.append(embedding)\n",
    "    embedded_abstracts_dict[key] = embedded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1984b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_abstracts_dict['周潇'][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "all_embeddings = []\n",
    "labels = []\n",
    "for label, tensors in embedded_abstracts_dict.items():\n",
    "    for tensor in tensors:\n",
    "        mean_embedding = tensor.mean(dim=1).squeeze(0).numpy()\n",
    "        all_embeddings.append(mean_embedding)\n",
    "        labels.append(label)\n",
    "embeddings_array = np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "all_embeddings = []\n",
    "labels = []\n",
    "for label, tensors in embedded_abstracts_dict.items():\n",
    "    for tensor in tensors:\n",
    "        all_embeddings.append(tensor)\n",
    "        labels.append(label)\n",
    "embeddings_array = np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53801c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2,init='pca')\n",
    "reduced_embeddings = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "label_to_color = {label: color for label, color in zip(unique_labels, colors)}\n",
    "\n",
    "for label in unique_labels:\n",
    "    indices = [i for i, l in enumerate(labels) if l == label]\n",
    "    plt.scatter(reduced_embeddings[indices, 0], reduced_embeddings[indices, 1], label=label, color=label_to_color[label])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b6314d",
   "metadata": {},
   "source": [
    "## 可以看到，效果很差，几乎等于没做，大家的研究方向虽然相似，但是整篇段落摘要进行嵌入，粒度太粗，所以还是打算基于关键主题和词topic来做"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc291a5",
   "metadata": {},
   "source": [
    "# Kmeans聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b423b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "n = 50\n",
    "SSE = []  # 存放每次结果的误差平方和\n",
    "for k in range(1,n):\n",
    "    estimator = KMeans(n_clusters=k)  # 构造聚类器\n",
    "    estimator.fit(embeddings_array)\n",
    "    SSE.append(estimator.inertia_)\n",
    "X = range(1,n)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "plt.plot(X,SSE,'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "Scores = []  # 存放轮廓系数\n",
    "for k in range(2,50):\n",
    "    estimator = KMeans(n_clusters=k)  # 构造聚类器\n",
    "    estimator.fit(embeddings_array)\n",
    "    Scores.append(silhouette_score(embeddings_array,estimator.labels_,metric='euclidean'))\n",
    "X = range(2,50)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('轮廓系数')\n",
    "plt.plot(X,Scores,'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e084912",
   "metadata": {},
   "source": [
    "### 由图可见最佳聚类就是2类，至少在该模型的结果下\n",
    "### 接下来就需要研究该模型的分类效果是否正常，即查看在该分类下，两类论文是否呈现明显不同的主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c562f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_num = 2\n",
    "\n",
    "estimator = KMeans(n_clusters=cluster_num,random_state=0)\n",
    "estimator.fit(embeddings_array)\n",
    "clusers = []\n",
    "for i in range(cluster_num):\n",
    "    clusers.append([])\n",
    "\n",
    "for index in range(len(estimator.labels_)):\n",
    "    clusers[estimator.labels_[index]].append(article_list[index])\n",
    "\n",
    "n = 1\n",
    "for articles in clusers:\n",
    "    df = pd.DataFrame(articles, columns=['articles'])\n",
    "    df.to_excel(f\"article_class_{n}.xlsx\", index=False)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21987ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ffa66a8",
   "metadata": {},
   "source": [
    "## 使用sci-bert和Key-bart方法 (不同于bert 使用了编码器-解码器结构，bert只使用编码器，更适合于txt2txt的文本生成，抽取论文主题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2625f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r\"A:\\新下载\\1712840133188.png\",width=1000,height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('C:\\\\Users\\\\54758\\\\bert\\\\sic-bert')\n",
    "model = AutoModel.from_pretrained('C:\\\\Users\\\\54758\\\\bert\\\\sic-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9515287",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"C:\\\\Users\\\\54758\\\\bert\\\\keybart\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"C:\\\\Users\\\\54758\\\\bert\\\\keybart\")\n",
    "# 数据\n",
    "data=cleaned_abstracts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0536d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a94fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, tokenizer, model, device):\n",
    "    inputs = tokenizer.encode(\"extract keywords: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model.generate(inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "    outputs = outputs.to('cpu')\n",
    "    keywords = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    keywords_list = keywords.split(',')\n",
    "    return keywords_list\n",
    "for scholar, papers in data.items():\n",
    "    print(f\"学者：{scholar}\")\n",
    "    for paper in papers:\n",
    "        keywords = extract_keywords(paper, tokenizer, model,device)\n",
    "        print(f\"论文关键词：{keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822eb3ec",
   "metadata": {},
   "source": [
    "## 这个模型似乎并不能很好处理中文，中文得想办法使用其他方法，先搞英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771406a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_chinese(text):\n",
    "    return any('\\u4e00' <= char <= '\\u9fff' for char in text)# 检查文本是否包含中文字符\n",
    "def extract_keywords2(text, tokenizer, model, device):\n",
    "    if contains_chinese(text):\n",
    "        return []#去除中文部分\n",
    "    inputs = tokenizer.encode(\"extract keywords: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    outputs = model.generate(inputs, max_length=50, num_beams=5, early_stopping=True).to('cpu')\n",
    "    keywords = tokenizer.decode(outputs[0], skip_special_tokens=True).split(',')\n",
    "    return keywords\n",
    "\n",
    "scholars_keywords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "for scholar, papers in data.items():\n",
    "    all_keywords = []\n",
    "    for paper in papers:\n",
    "        keywords = extract_keywords2(paper, tokenizer, model, device)\n",
    "        all_keywords.extend(keywords) \n",
    "    scholars_keywords[scholar] = all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scholar, keywords in scholars_keywords.items():\n",
    "#     keyword_counts = Counter(keywords)\n",
    "#     sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "#     print(f\"学者：{scholar}\")\n",
    "#     for keyword, freq in sorted_keywords:\n",
    "#         print(f\"{keyword}: {freq}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10869f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scholar, keywords in scholars_keywords.items():\n",
    "    all_keywords = [keyword for i in keywords for keyword in i.split(';') if keyword]  # 分割每个字符串，忽略空字符串\n",
    "    keyword_counts = Counter(all_keywords)\n",
    "    sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"学者：{scholar}\")\n",
    "    for keyword, freq in sorted_keywords:\n",
    "        print(f\"{keyword}: {freq}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ddc19",
   "metadata": {},
   "source": [
    "### 虽然都提取出来了，但每个的词频似乎都不太高，拿来聚类效果一般不明显"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae173a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
